{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPtmPexK2DxUStf0w1fdVNV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ayon150/AI/blob/main/Mamba_code_to_Mnist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IlTCpNggd4RE",
        "outputId": "f45cbb04-4d31-4481-ef1f-9a05a76a39f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Collecting mamba-ssm\n",
            "  Downloading mamba_ssm-2.2.6.post3.tar.gz (113 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/113.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.9/113.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install required packages (run in Colab or your environment)\n",
        "!pip install torch torchvision mamba-ssm pytorch_lightning\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import pytorch_lightning as pl\n",
        "from mamba_ssm import MambaLayer  # example import, refer to the specific API\n",
        "\n",
        "# 1. Prepare MNIST dataset (even vs odd)\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset  = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Convert labels to even=0, odd=1\n",
        "train_dataset.targets = (train_dataset.targets % 2).long()\n",
        "test_dataset.targets  = (test_dataset.targets  % 2).long()\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=64, shuffle=False, num_workers=4)\n",
        "\n",
        "# 2. Define a simple model using MambaLayer + classification head\n",
        "class MNIST_Mamba_Model(pl.LightningModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # e.g., flatten 28x28 into sequence 784 and feed into Mamba-SSM\n",
        "        self.seq_len = 28*28\n",
        "        self.input_dim = 1\n",
        "        self.mamba = MambaLayer(d_model=64, seq_len=self.seq_len)\n",
        "        self.fc1   = nn.Linear(64, 32)\n",
        "        self.fc_out= nn.Linear(32, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [batch, 1, 28, 28]\n",
        "        b = x.size(0)\n",
        "        x = x.view(b, self.seq_len, self.input_dim)  # [batch, seq_len, input_dim]\n",
        "        y = self.mamba(x)                             # [batch, seq_len, d_model]\n",
        "        y = y.mean(dim=1)                             # pool sequence dimension\n",
        "        y = F.relu(self.fc1(y))\n",
        "        y = torch.sigmoid(self.fc_out(y)).squeeze(1)\n",
        "        return y\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_hat = self(x)\n",
        "        loss  = F.binary_cross_entropy(y_hat, y.float())\n",
        "        acc   = ((y_hat>0.5).long()==y).float().mean()\n",
        "        self.log('train_loss', loss, on_epoch=True)\n",
        "        self.log('train_acc',  acc, on_epoch=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_hat = self(x)\n",
        "        loss  = F.binary_cross_entropy(y_hat, y.float())\n",
        "        acc   = ((y_hat>0.5).long()==y).float().mean()\n",
        "        self.log('val_loss', loss, on_epoch=True, prog_bar=True)\n",
        "        self.log('val_acc',  acc, on_epoch=True, prog_bar=True)\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_hat = self(x)\n",
        "        loss  = F.binary_cross_entropy(y_hat, y.float())\n",
        "        acc   = ((y_hat>0.5).long()==y).float().mean()\n",
        "        self.log('test_loss', loss)\n",
        "        self.log('test_acc',  acc)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=1e-4)\n",
        "\n",
        "# 3. Train the model\n",
        "model = MNIST_Mamba_Model()\n",
        "trainer = pl.Trainer(max_epochs=10, gpus=1 if torch.cuda.is_available() else 0)\n",
        "trainer.fit(model, train_loader, valid_dataloaders=test_loader)\n",
        "trainer.test(model, test_dataloaders=test_loader)\n"
      ]
    }
  ]
}